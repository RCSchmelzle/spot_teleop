

### High-level pipeline

For K cameras, reference camera is cam0 (e.g. `camA`).

1. **Run ORB-SLAM3 RGBD per camera on a calibration bag.**

   * Use `xtionA_rgbd.yaml` as canonical (Xtion intrinsics + DepthMapFactor 1.0).
   * Remap:

     * `/camera/rgb` → `/camX/rgb/image_raw`
     * `/camera/depth` → `/camX/depth/image`
   * Save `KeyFrameTrajectory_camX.txt` for each camera.

2. **Parse trajectories into SE(3) time series.**

   * For each camera k:

     * Parse TUM-format file into a sorted list of:

       * `t` (float seconds)
       * `T` (4×4 SE3 matrix)

3. **Implement pose bracketing.**

   * For a trajectory list and query time `t_query`, implement:

     ```python
     def bracket_pose(traj, t_query) -> Optional[(PoseSE3, PoseSE3)]
     ```

   * Returns closest before & after poses or `None`.

4. **Compute shared valid timestamps τ_j (automatic time alignment).**

   * For all k, find overlapping interval `[t_min, t_max]`.
   * Choose sampling step Δt_sample ~ 1/30 or 1/20.
   * Generate τ_j in `[t_min, t_max]`.
   * For each τ_j and each camera k:

     * Get `(before, after) = bracket_pose(traj_k, τ_j)`.
     * If any missing → reject τ_j.
     * Compute `dt_before`, `dt_after`.
     * Require: `dt_before <= εₜ` **and** `dt_after <= εₜ`, with εₜ = 0.05 s.
   * Keep only τ_j where **all** cameras pass.

5. **Interpolate SE(3) poses at each τ_j.**

   * For each τ_j and each camera k:

     * Compute interpolation factor α.
     * Interpolate translation linearly.
     * Interpolate rotation via quaternion slerp.
     * Build `T_camk(τ_j)`.

6. **Build point correspondences per (reference camera, camera k).**

   * Reference camera: k=0.
   * For each non-ref camera k:

     * For each valid τ_j:

       * `X_j = position of cam0 at τ_j`
       * `Y_j = position of camk at τ_j`
     * Build Nx3 sets `X`, `Y`.

7. **Run fixed-scale Umeyama to find rigid transform.**

   * For each camera k>0:

     * Solve:

       [
       X_j \approx R Y_j + t
       ]

       with **scale fixed to 1**.
     * Get SE3:

       ```python
       T_world0_worldk = [[R, t], [0,0,0,1]]
       ```

   * This transforms camera k’s SLAM world into reference camera’s SLAM world (full rotation + translation).

8. **Save extrinsics.**

   * For each camera k>0:

     * Save R, t (or 4×4) in a YAML config, e.g.:

       ```yaml
       cam0:
         ...
       cam1:
         R: [...]
         t: [...]
       ```

   * Use consistent naming and format for N cameras. Have in a single file that keeps all poses

9. **Integrate into the interactive bag calibrator.**

   * In the calibrator UI / CLI:

     1. List available bags in `datasets/`.
     2. Let user pick a bag.
     3. For each camera: run ORB-SLAM3 RGBD, save trajectories.
     4. Run steps 2–8.
     5. Show extrinsics and basic stats (number of τ_j used, RMS error).
     6. Save results to a known config path and log.

10. **Add “live bringup + record + calibrate” option.**

    * In the calibrator, along with “choose from existing bags”, add:

      > “Record new calibration bag from live Xtions and run calibration”

    * Behavior:

      1. Launch Xtions (camA, camB, …) via `xtion_bringup`.
      2. Start rosbag record to `datasets/xtion_calib_YYYYMMDD_HHMMSS.bag` including:

         * `/cam*/rgb/image_raw`, `/cam*/depth/image`, `/cam*/rgb/camera_info`
      3. Ask user to move the rig (parallax, rotation).
      4. Stop recording.
      5. Automatically run full calibration on this **most recent** bag using steps 1–9.

    * Add / rename a script like:

      ```bash
      ./extrinsic_camera_calibration.sh
      ```

      with header:

      ```bash
      # Calibrates rigid camera extrinsics using ORB-SLAM3 trajectories
      # and multi-trajectory time alignment + Umeyama alignment.
      ```

---





3 add zed compatibility and config files and incorporate into the above; test

4 upload to the mini pc and ensure we can bringup and record on zed and xtion

5 mount on new one with computer plugged into outlet on extension cord on spot; have hdmi to verify beforehand

6 run the calibration with spot walking around with the mounted cameras; can do prior record only variant that jsut records 


7 and then do alignment after for more accuracy, move to my computer for processing 
 
8 work on electric for spot for truly mobile

9 set up spot to start remote record from computer and use laptop to control spot and contorlling and getting the remote feed

10 gui setup 
